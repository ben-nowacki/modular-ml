{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3772275a",
   "metadata": {},
   "source": [
    "---\n",
    "# 01_featureset_basics.ipynb\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42e4df",
   "metadata": {},
   "source": [
    "## What is a FeatureSet?\n",
    "\n",
    "In ModularML, the `FeatureSet` class defines the core data structure used throughout the modeling pipeline. \n",
    "It wraps raw input data (like NumPy arrays or dictionaries) and organizes them into clearly separated components:\n",
    "* **Features**: the inputs to the model\n",
    "* **Targets**: the the desired outputs or labels to predict\n",
    "* **Tags**: metadata used to identify, filter, and group samples (e.g., cell ID, temperature, cycling condition)\n",
    "\n",
    "This notebook introduces the creation and manipulation of `FeatureSet` objects using real-world battery diagnostic data. \n",
    "We'll demonstrate how to:\n",
    "* Load raw data\n",
    "* Build a FeatureSet from a dictionary\n",
    "* Define features, targets, and tags\n",
    "* Split the data into training/validation/testing subsets\n",
    "* Apply feature transforms for normalization or scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611971a",
   "metadata": {},
   "source": [
    "## Step 1: Loading Example Data\n",
    "\n",
    "We begin by downloading an example dataset from our battery health estimation paper: “Fine-tuning for rapid capacity estimation of lithium-ion batteries” ([10.1016/j.ensm.2025.104425](https://doi.org/10.1016/j.ensm.2025.104425)).\n",
    "This dataset contains time-series voltage responses to 100-second DC pulses collected across the life of several lithium-ion cells.\n",
    "More information on the dataset and usage can be found on the following GitHub repository: [REIL-UConn/fine-tuning-for-rapid-soh-estimation]{https://github.com/REIL-UConn/fine-tuning-for-rapid-soh-estimation.git}.\n",
    "\n",
    "\n",
    "We'll download and load the data from GitHub. \n",
    "The file is a `.pkl` (pickled Python object) containing a dictionary of key-value arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import urllib.request\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import modularml as mml\n",
    "\n",
    "DATA_URL = \"https://raw.githubusercontent.com/REIL-UConn/fine-tuning-for-rapid-soh-estimation/main/processed_data/UConn-ILCC-NMC/data_slowpulse_1.pkl\"\n",
    "DATA_DIR = Path(\"downloaded_data\")\n",
    "DATA_PATH = DATA_DIR / \"data_slowpulse_1.pkl\"\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    print(\"Downloading data...\")\n",
    "    urllib.request.urlretrieve(url=DATA_URL, filename=DATA_PATH)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Data already downloaded.\")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "    data = pickle.load(Path.open(DATA_PATH, \"rb\"))\n",
    "\n",
    "print(f\"Available keys: {list(data.keys())}\")\n",
    "print(f\"Total number of samples: {len(data[next(iter(data.keys()))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a72afc",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the Data Format\n",
    "\n",
    "Our data is structured as a dictionary, where each key corresponds to a signal or property:\n",
    "* `voltage`: the time-series voltage response during a 100-second pulse (shape: [n_samples, 101])\n",
    "* `soh`: state-of-health, a float between ~0.5 and 1.0\n",
    "* `cell_id`, `group_id`, `pulse_type`, `soc`: metadata about the sample\n",
    "\n",
    "We'll now map these to the correct FeatureSet roles:\n",
    "* **Feature**: `voltage`\n",
    "* **Target**: `soh`\n",
    "* **Tags**: `cell_id`, `group_id`, `pulse_type`, `pulse_soc`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce70c3",
   "metadata": {},
   "source": [
    "## Step 3: Creating a FeatureSet\n",
    "\n",
    "To create a `FeatureSet`, we use the `from_dict()` constructor. \n",
    "We must assign a `label`, which uniquely identifies this `FeatureSet` within the broader modeling pipeline (e.g., when connecting to model stages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularml.core import FeatureSet\n",
    "\n",
    "fs = FeatureSet.from_dict(\n",
    "    label=\"PulseFeatures\",\n",
    "    data={\n",
    "        \"voltage\": data[\"voltage\"],\n",
    "        \"soh\": data[\"soh\"],\n",
    "        \"cell_id\": data[\"cell_id\"],\n",
    "        \"group_id\": data[\"group_id\"],\n",
    "        \"pulse_type\": data[\"pulse_type\"],\n",
    "        \"pulse_soc\": data[\"soc\"],\n",
    "    },\n",
    "    feature_keys=\"voltage\",\n",
    "    target_keys=\"soh\",\n",
    "    tag_keys=[\"cell_id\", \"group_id\", \"pulse_type\", \"pulse_soc\"],\n",
    ")\n",
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f848d6d",
   "metadata": {},
   "source": [
    "## Step 4: Exploring Tags and Filtering\n",
    "\n",
    "Tags let us group, filter, and analyze our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = fs.get_all_tags(fmt=mml.DataFormat.PANDAS)\n",
    "print(\"Unique cell IDs:\", df_tags[\"cell_id\"].unique())\n",
    "print(\"Pulse types:\", df_tags[\"pulse_type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b7b26",
   "metadata": {},
   "source": [
    "We can easily filter to only samples matching specific tag values.\n",
    "\n",
    "The `.filter` method takes keyword arguments, where keys can correspond to any attribute of the samples' tags, features, or targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples = fs.filter(pulse_type=\"chg\")\n",
    "print(charge_samples)\n",
    "print(f\"Filtered to {len(charge_samples.samples)} charge-only samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bbaf7",
   "metadata": {},
   "source": [
    "Note that the `.filter` method returns a new `FeatureSet` containing copies of the filtered samples.\n",
    "\n",
    "By default, it is returned with a label of `'filtered'`, but we can set a new label with `.set_label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93066897",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.label = \"ChargePulses\"\n",
    "print(charge_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39673cbf",
   "metadata": {},
   "source": [
    "## Step 5: Splitting Into Train/Val/Test\n",
    "\n",
    "To train a machine learning model properly, we must divide our data into non-overlapping subsets. \n",
    "\n",
    "ModularML provides several built-in splitting methods:\n",
    "* `FeatureSet.split_random()`: creates subsets by assigning random proportions of all samples to each split\n",
    "* `FeatureSet.split_by_condition()`: creates subsets using any number of explicit conditions\n",
    "\n",
    "For example, a random 50% train, 30% valdation, and 20% test split can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3df113",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.split_random(ratios={\"train\": 0.5, \"val\": 0.3, \"test\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbacfa9",
   "metadata": {},
   "source": [
    "Note that the `.split_...` methods of FeatureSet return the created FeatureSubsets in addition to storing them internally. \n",
    "\n",
    "We can validate that these subsets are retained in FeatureSet using the `.available_subsets` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.available_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0f31e",
   "metadata": {},
   "source": [
    "Normally, when splitting data, you need to be more specific with the segmentation.\n",
    "\n",
    "In our case, we need to ensure each split contains at a minimum, unique cell IDs (`'cell_id'`).\n",
    "Having pulse responses from the same cell in both the training and test sets would lead to data leakage and introduce bias into the model’s performance.\n",
    "\n",
    "To further strengthen our evaluation, we can split by cycling conditions (`'group_id'`), ensuring that the test set includes only battery cycling scenarios that the model thas never encountered during training.\n",
    "This approach provides a clearer picture of the model’s ability to generalize to unseen conditions, which is essential for assessing its suitability for real-world deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b24330",
   "metadata": {},
   "source": [
    "To achieve this type of segementation, we have a couple options:\n",
    "\n",
    "1. Utilize the `group_by` argument of `split_random`\n",
    "\t- If we set `group_by='group_id'`, then all samples are first grouped into like cycling groups before randomly selecting for subsets\n",
    "\t- This is essentially saying \"take a random selection of `'group_id'` with the defined ratios\"\n",
    "\n",
    "2. Provide explicit grouping conditions with `split_by_condition`\n",
    "\t- This splitting method allows for defining a set of conditions applied to any key within each sample. \n",
    "\t- For example, we could split into subsets based on the `'group_id'` value:\n",
    "\t\t``` python\n",
    "\t\tcharge_samples.split_by_condition(train={'group_id': lambda x: x < 5}, val=..., ...)\n",
    "\t\t```\n",
    "\t- This assigns all cycling groups with IDs < 5 to the `train` set.\n",
    "\n",
    "\n",
    "For our case, where we only need to account for a single tag (`'group_id'`), we can just use `split_random`.\n",
    "\n",
    "However, `split_by_condition` becomes very powerful when you need segmentation across multiple tags. \n",
    "Say for example you wanted to add another segmentation based on the sample's state of health (`'soh'`).\n",
    "Then we could do:\n",
    "\n",
    "``` python\n",
    "\tcharge_samples.split_by_condition(\n",
    "\t\ttrain={\n",
    "\t\t\t'group_id': [1,2,3],\n",
    "\t\t\t'soh': lambda x: x >= 90.0,\n",
    "\t\t},\n",
    "\t\tval=..., \n",
    "\t\t...\n",
    "\t)\n",
    "```\n",
    "Now the `train` subset contains specific cycling groups and only the early-life data, where state-of-health is above 90%. \n",
    "It becomes incredibly easy to format the FeatureSet for advanced model training and evaluations.\n",
    "\n",
    "Let's get back to our example ...\n",
    "\n",
    "First we'll have to remove the subsets created previously. \n",
    "This is easily achieved with the `clear_subsets` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.clear_subsets()\n",
    "charge_samples.available_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fa5e4",
   "metadata": {},
   "source": [
    "Next, we'll create subsets with the same {50% train, 30% validation, and 20% test} split ratio, but now we'll ensure unique cycling conditions in each set using the `group_by` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.split_random(\n",
    "    ratios={\"train\": 0.5, \"val\": 0.3, \"test\": 0.2},\n",
    "    group_by=\"group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095e573",
   "metadata": {},
   "source": [
    "Since there are only 11 unique groups, the resulting ratios won't be perfect, but we can check how close it got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45631f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for subset_label, subset in charge_samples.subsets.items():\n",
    "    msg = f\"{subset_label}: n={len(subset)} ({len(subset) / len(charge_samples) * 100:.01f}% of all samples)\\n\"\n",
    "    msg += f\"  > Unique group_ids: {np.unique(subset.get_all_tags()['group_id'])}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db9f23",
   "metadata": {},
   "source": [
    "Great! We now have `train`, `val`, and `test` subsets, each with a unique set of cycling conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242fbba",
   "metadata": {},
   "source": [
    "## Step 6: Feature Transforms\n",
    "\n",
    "To make model training more stable, it's common to normalize input features. \n",
    "ModularML provides this functionality with a `FeatureTranform` class that wrap common normalization/scaling methods like those from the `sklearn.preprocessing` module. \n",
    "It also defines a few custom ones. \n",
    "\n",
    "We can view the list of available transform using the `get_supported_scalers` method.\n",
    "\n",
    "This provides the tranform name and corresponding class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04dbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularml.core import FeatureTransform\n",
    "\n",
    "FeatureTransform.get_supported_scalers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbff08",
   "metadata": {},
   "source": [
    "Before we start applying transforms, let's look at our data. \n",
    "\n",
    "Several data accessor methods are provided on FeatureSets including:\n",
    "\n",
    "* `get_all_features(format:modularML.DataFormat)`: returns all features values from all samples in FeatureSet\n",
    "\n",
    "* `get_all_targets(format:modularML.DataFormat)`: returns all target values from all samples in FeatureSet\n",
    "\n",
    "* `get_all_tags(format:modularML.DataFormat)`: returns all tags values from all samples in FeatureSet\n",
    "\n",
    "\n",
    "The `format` argument allows the data to be returned in any specified format (e.g., Pandas DataFrame, dict of lists, numpy arrays, etc). \n",
    "See the documentation for more details on format types. \n",
    "\n",
    "\n",
    "Let's make a quick plot of the underlying `voltage` feature in each subset.\n",
    "We'll wrap it in a function so we can plot it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb911db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_voltages(fs: FeatureSet, n_samples: int = 200, seed: int = 13):\n",
    "    \"\"\"\n",
    "    Plot the 'voltage' feature contained in the FeatureSet.\n",
    "\n",
    "    Each subset will get its own panel.\n",
    "    Colors by SOH (dark blue = high SOH, light blue = low SOH)\n",
    "\n",
    "    Args:\n",
    "        fs (FeatureSet): FeatureSet to use.\n",
    "        n_samples (int, optional): The number of samples in `fs` that will\n",
    "              get plotted. Defaults to 200.\n",
    "        seed (int, optional): A seed to ensure the same samples get plotted\n",
    "              with repeated calls. Defaults to 13.\n",
    "\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    scm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=50, vmax=100))\n",
    "\n",
    "    # Create figure with panels for each subset\n",
    "    fig, axes = plt.subplots(figsize=(10, 3), ncols=fs.n_subsets, sharex=True, sharey=True)\n",
    "    for i, (subset_label, subset) in enumerate(fs.subsets.items()):\n",
    "        # For each subset, get all voltage features and group_ids\n",
    "        voltages = subset.get_all_features(fmt=mml.DataFormat.DICT_NUMPY)[\"voltage\"]\n",
    "        sohs = subset.get_all_targets(fmt=mml.DataFormat.DICT_NUMPY)[\"soh\"]\n",
    "\n",
    "        # Select n_samples\n",
    "        sample_idxs = rng.choice(np.arange(0, len(voltages)), size=n_samples)\n",
    "        for idx in sample_idxs:\n",
    "            axes[i].plot(voltages[idx], color=scm.to_rgba(sohs[idx]))\n",
    "\n",
    "        axes[i].set_title(subset_label)\n",
    "        axes[i].set_xlabel(\"Time (s)\")\n",
    "    axes[0].set_ylabel(\"Voltage (V)\")\n",
    "\n",
    "    # Adjust main subplot area to leave space on the right for colorbar\n",
    "    fig.tight_layout(pad=1)\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "\n",
    "    # Add colorbar as a dedicated panel on the far right\n",
    "    cbar_ax = fig.add_axes([0.87, 0.19, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(scm, cax=cbar_ax)\n",
    "    cbar.set_label(\"SOH (%)\")\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "fig, axes = plot_voltages(charge_samples, n_samples=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c147d4",
   "metadata": {},
   "source": [
    "Let's apply a MinMaxScaler to ensure all data is in the range [0, 1].\n",
    "\n",
    "Applying transform to FeatureSet object is performed with the `fit_transform` method.\n",
    "It requires three arguments:\n",
    "\n",
    "* `fit`: a string specifying on which segment of the FeatureSet the transform should be fit to\n",
    "* `apply`: a string specifying to which segment of the FeatureSet the transform should be applied to\n",
    "* `transform`: a `FeatureTransform` instance defining the transformation\n",
    "\n",
    "\n",
    "The `fit` and `apply` strings can take the form of `'{subset}.{component}.{key}'` where\n",
    "* `component` is the only required value and must be either `'features'` or `'targets'`\n",
    "  \t* If `subset` is omitted (e.g., `fit='features'`), then the transform is fit to features from all subsets.\n",
    "* `key` can be a specific feature or target (e.g., just the `'voltage'` feature)\n",
    "\n",
    "\n",
    "A `FeatureTransform` can be defined with an instance of a supported scaler/transform (e.g., `sklearn::StandardScaler`) or with the name of the scaler, as provided in `FeatureTransform.get_supported_scalers().keys()`. For example, the following are equivalent:\n",
    "``` python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "FeatureTransform(StandardScaler())\n",
    "FeatureTransform('standard')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845b93a",
   "metadata": {},
   "source": [
    "To prevent data leakage, we only want to fit our transform to the `train` subset, but apply it to all subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.fit_transform(\n",
    "    fit=\"train.features\",\n",
    "    apply=\"features\",\n",
    "    transform=FeatureTransform(\"minmax\"),\n",
    ")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f21546",
   "metadata": {},
   "source": [
    "Say we don't like that transform and want to apply a different one.\n",
    "We can remove all transforms on our data by using `undo_all_transforms`. \n",
    "> Note that we can optionally provide an `on` argument, which defines whether to undo all transforms (`on=None`, default), all feature transforms (`on='features'`), or all target transforms (`on='targets'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.undo_all_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3fae7",
   "metadata": {},
   "source": [
    "Our feature signals start at random voltage levels, but maybe we know that health information is encoded in the curvature of the pulse response (e.g., the rise/decay rate in the middle portion of the voltage reading).\n",
    "\n",
    "In this case, we might want to shift all samples so they start at a value of 0. This is achieved with the `'sample_zero'` or `PerSampleZeroStart` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularml.preprocessing import MinMaxScaler, PerSampleZeroStart\n",
    "\n",
    "charge_samples.fit_transform(\n",
    "    fit=\"train.features\",\n",
    "    apply=\"features\",\n",
    "    transform=FeatureTransform(PerSampleZeroStart()),\n",
    ")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ec0bb",
   "metadata": {},
   "source": [
    "Now we might want to reapply the MinMax scaler to ensure all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.fit_transform(\n",
    "    fit=\"train.features\",\n",
    "    apply=\"features\",\n",
    "    transform=FeatureTransform(MinMaxScaler()),\n",
    ")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ce25e",
   "metadata": {},
   "source": [
    "We can undo a single transform at a time using the `undo_last_transform` method.\n",
    "\n",
    "Here, we remove on the last 'minmax' transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.undo_last_transform(on=\"features\")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019f3f4",
   "metadata": {},
   "source": [
    "With the tracked application of transforms, it becomes very easy to experiment with different feature preprocessing techniques. \n",
    "\n",
    "We can even create customs transforms. \n",
    "Any object can be passed to `FeatureTransform(scaler=...)` so long as it defines a `fit()`, `tranform()`, and `inverse_tranform()` method.\n",
    "\n",
    "For example, we see that the voltage response can be broken into three distinct segments: (0, 30) seconds, (30,40) seconds, and (40,100) seconds. Maybe we want to create a scaler that create independent scalers for each of those segments.\n",
    "\n",
    "Let's create `MyScaler` that does just that. On initiallization it takes any scaler, that will be duplicated and fit to each defined segment. If we use the `PerSampleZeroStart` for each segment, we can shift the start of each segment to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36593616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularml.preprocessing import MinMaxScaler, PerSampleZeroStart\n",
    "\n",
    "\n",
    "class MyScaler:\n",
    "    def __init__(self, scaler=None):\n",
    "        self.scaler = scaler if scaler is not None else MinMaxScaler()\n",
    "        self._segment_scalers = []\n",
    "        self._boundaries = [0, 31, 41, 101]\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        # X has shape: (n_samples, feature_len)\n",
    "        self._segment_scalers.clear()\n",
    "        for i in range(len(self._boundaries) - 1):\n",
    "            start, end = self._boundaries[i], self._boundaries[i + 1]\n",
    "            segment = X[:, start:end]\n",
    "            scaler = self.scaler.__class__(**self.scaler.get_params())  # clone scaler\n",
    "            scaler.fit(segment)\n",
    "            self._segment_scalers.append(scaler)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        segments = []\n",
    "        for i, scaler in enumerate(self._segment_scalers):\n",
    "            start, end = self._boundaries[i], self._boundaries[i + 1]\n",
    "            segment = X[:, start:end]\n",
    "            transformed = scaler.transform(segment)\n",
    "            segments.append(transformed)\n",
    "        return np.concatenate(segments, axis=1)\n",
    "\n",
    "    def inverse_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        segments = []\n",
    "        for i, scaler in enumerate(self._segment_scalers):\n",
    "            start, end = self._boundaries[i], self._boundaries[i + 1]\n",
    "            segment = X[:, start:end]\n",
    "            inverse = scaler.inverse_transform(segment)\n",
    "            segments.append(inverse)\n",
    "        return np.concatenate(segments, axis=1)\n",
    "\n",
    "\n",
    "charge_samples.fit_transform(\n",
    "    fit=\"train.features\",\n",
    "    apply=\"features\",\n",
    "    transform=FeatureTransform(MyScaler(PerSampleZeroStart())),\n",
    ")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ba309",
   "metadata": {},
   "source": [
    "And now if we apply normal MinMax scaling, we start to see a strong separation of feature values with different states of health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.fit_transform(\n",
    "    fit=\"train.features\",\n",
    "    apply=\"features\",\n",
    "    transform=FeatureTransform(MinMaxScaler()),\n",
    ")\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333c82b",
   "metadata": {},
   "source": [
    "Note that a `SegmentedScaler` class is provided in `modularml.preprocessing` that does the same things as `MyScaler`, except it allows for configurable boundaries and provides extra error checking.\n",
    "\n",
    "\n",
    "Unfortunately, full serialization of custom scalers is not yet supported.\n",
    "In order to reuse this FeatureSet (and underlying transforms), we'll switch to the built-in `SegmentedScaler` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modularml.preprocessing import SegmentedScaler\n",
    "\n",
    "charge_samples.undo_all_transforms()\n",
    "\n",
    "scaler = SegmentedScaler(\n",
    "    boundaries=[0, 31, 41, 101],\n",
    "    scaler=PerSampleZeroStart(),\n",
    ")\n",
    "charge_samples.fit_transform(fit=\"train.features\", apply=\"features\", transform=FeatureTransform(scaler))\n",
    "charge_samples.fit_transform(fit=\"train.features\", apply=\"features\", transform=FeatureTransform(MinMaxScaler()))\n",
    "\n",
    "fig, axes = plot_voltages(charge_samples)\n",
    "plt.show()\n",
    "\n",
    "# Add min-max scaling to targets\n",
    "charge_samples.fit_transform(fit=\"train.targets\", apply=\"targets\", transform=FeatureTransform(MinMaxScaler()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cee89c",
   "metadata": {},
   "source": [
    "When using built-in transforms, `FeatureSet` instances are fully seriallizable, meaning that subsets, transforms, and underlying sample data can be saved and restored with the `save` and `load` methods.\n",
    "\n",
    "Let's save our pre-processed `charge_samples` FeatureSet for use in later example notebooks. We'll put in the same folder as the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877209d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_samples.save(DATA_DIR / \"charge_samples\", overwrite_existing=True)\n",
    "\n",
    "# Reloading the feature set can be achieved with:\n",
    "# fs_reload = FeatureSet.load(DATA_DIR / \"charge_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d1564",
   "metadata": {},
   "source": [
    "This concludes the **01_featureset_basics** notebook.\n",
    "\n",
    "The next tutorial covers ModelGraph construction: [02_modelgraph_basics.ipynb](./02_modelgraph_basics.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
